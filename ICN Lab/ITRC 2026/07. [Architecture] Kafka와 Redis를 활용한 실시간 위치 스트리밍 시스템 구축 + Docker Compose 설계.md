### Kafka와 Redis로 구축하는 대용량 실시간 위치 스트리밍 아키텍처
> 🔗 **관련 PR**: [Kafka, Redis 기반 실시간 위치 스트리밍 인프라 및 기능 구현](https://github.com/ICN-Soongsil/LBS-project/pull/6)
>

> 사용자로부터 쏟아지는 대량의 위치 정보를 어떻게 하면 유실 없이, 그리고 실시간으로 처리할 수 있을까?
ICN 연구 과제를 수행하며 모빌리티 데이터의 실시간 성을 보장하기 위한 백엔드 구조를 고민했다. 단순히 관계형 데이터베이스에 위치를 기록하는 방식은 급증하는 트래픽과 빈번한 쓰기 요청을 감당하기에 한계가 명확했다. 이를 해결하기 위해 Kafka를 메시지 브로커로 활용하여 시스템 간 결합도를 낮추고, Redis의 공간 인덱싱 기능을 통해 초저지연 위치 조회가 가능한 시스템을 구축했다. 입구부터 저장소까지 데이터가 흐르는 파이프라인을 설계하며 고려한 기술적 의사결정 과정들을 정리한다.
>
---


**리팩토링 요약**

- 배경: 대량의 실시간 위치 데이터 유입에 따른 시스템 부하 분산 필요성
- 핵심: Kafka를 통한 이벤트 기반 아키텍처 및 Redis Geo 기능을 활용한 공간 데이터 관리
- 결과: 비동기 처리를 통한 응답 속도 최적화 및 실시간 위치 기반 서비스 확장성 확보

</br></br>

### 1. 인프라 설정: 데이터가 흐르는 고속도로와 통역사

<img width="8192" height="2070" alt="ICN-kafka_redis-2026-01-27-080037" src="https://github.com/user-attachments/assets/b96caae2-c5e9-4e9c-baed-a425e8d57429" />

시스템이 안정적으로 돌아가려면 먼저 데이터가 이동할 통로와 이를 해석할 규칙이 필요하다.

</br>

**Step 1. Kafka를 통한 통로 확보 (KafkaConfig.java)**

Kafka 브로커에 위치 데이터를 실어 나를 토픽을 정의했다. 코드 기반 인프라 관리 방식을 채택하여 애플리케이션 실행 시 자동으로 토픽이 생성되도록 구성했다.

```java
@Configuration
public class KafkaConfig {
    @Bean
    public NewTopic locationTopic() {
        return TopicBuilder.name("location-events")
                .partitions(1)
                .replicas(1)
                .build();
    }
}
```

현재는 단일 브로커 환경에 맞춰 파티션과 복제본을 1개로 설정했으나, 향후 처리량 증설이 필요할 때 파티션 개수만 조정하면 수평적 확장이 가능한 구조를 갖추었다.

</br>

**Step 2. Redis를 위한 데이터 통역사 (RedisConfig.java)**

Redis는 기본적으로 바이트 배열로 데이터를 저장한다. Java 객체를 그대로 저장하면 사람이 읽기 어렵고 다른 시스템과 연동이 불편해지므로, 키는 문자열로, 값은 JSON 형식으로 변환하도록 직렬화 전략을 세웠다.

```java
@Configuration
public class RedisConfig {
    @Bean
    public RedisTemplate<String, Object> redisTemplate(RedisConnectionFactory connectionFactory) {
        RedisTemplate<String, Object> template = new RedisTemplate<>();
        template.setConnectionFactory(connectionFactory);
        template.setKeySerializer(new StringRedisSerializer());
        template.setValueSerializer(RedisSerializer.json());
        return template;
    }
}
```

</br></br>

### 2. 진입로 설계: 멈추지 않는 데이터 수집

사용자의 요청을 받는 컨트롤러와 이를 Kafka로 전달하는 프로듀서는 비차단 방식으로 동작해야 한다.

</br>

**Step 1. 비결합 아키텍처의 실현 (LocationController.java)**

컨트롤러는 데이터를 받자마자 Kafka로 던지고 즉시 성공 응답을 반환한다. 뒤쪽에서 데이터가 어떻게 처리되는지 기다리지 않는 이 구조는 사용자에게 최상의 응답 속도를 제공하며, 컨슈머에 장애가 발생하더라도 수집 단계의 병목을 방지한다.

```java
@Slf4j
@RestController
@RequestMapping("/api/v1/locations")
@RequiredArgsConstructor
public class LocationController {
    private final LocationProducer locationProducer;

    @PostMapping
    public ResponseEntity<String> receiveLocation(@Valid @RequestBody LocationRequest request) {
        log.info(">>> [Controller] 위치 정보 수신: userId={}", request.getUserId());
        locationProducer.sendLocation(request);
        return ResponseEntity.ok("Location event streaming has started.");
    }
}
```

</br>

**Step 2. 순서 보장과 비동기 발송 (LocationProducer.java)**

위치 정보는 발생 순서가 매우 중요하다. Kafka 전송 시 사용자 식별자를 메시지 키로 사용하여 특정 유저의 데이터가 항상 동일한 파티션에 쌓이도록 설계했다. 또한 콜백 함수를 사용하여 전송 성공 여부를 비동기적으로 기록함으로써 전체적인 처리 흐름을 방해하지 않도록 했다.

```java
@Slf4j
@Service
@RequiredArgsConstructor
public class LocationProducer {
    private final KafkaTemplate<String, Object> kafkaTemplate;
    private static final String TOPIC = "location-events";

    public void sendLocation(LocationRequest request) {
        if (request == null || request.getUserId() == null) return;

        kafkaTemplate.send(TOPIC, request.getUserId(), request)
                .whenComplete((result, ex) -> {
                    if (ex == null) {
                        log.info(">>> [Producer] 발송 성공: 유저={} | 파티션={}", 
                                 request.getUserId(), result.getRecordMetadata().partition());
                    } else {
                        log.error(">>> [Producer] 발송 실패: {}", ex.getMessage());
                    }
                });
    }
}
```

</br></br>

### 3. 데이터 처리 공장: Redis 공간 연산과 품질 관리

Kafka 토픽을 구독하는 컨슈머는 이 시스템에서 가장 복잡한 로직을 수행하는 핵심 엔진이다.

</br>

**Step 1. 공간 인덱싱과 데이터 생명 주기 (LocationConsumer.java)**

단순히 위경도 값을 저장하는 것에 그치지 않고 Redis의 Geo 기능을 활용했다. 이는 주변 기기 검색과 같은 공간 연산을 수 밀리초 내에 수행할 수 있게 한다. 또한 상세 상태 정보에는 30분의 만료 시간을 설정하여, 연결이 끊긴 유저의 유효하지 않은 데이터를 자동으로 정리하는 자원 관리 전략을 도입했다.

```java
@Slf4j
@Service
@RequiredArgsConstructor
public class LocationConsumer {
    private final RedisTemplate<String, Object> redisTemplate;
    private static final String GEO_KEY = "mobility:locations";
    private static final String STATUS_PREFIX = "mobility:status:";
    private static final Duration STATUS_TTL = Duration.ofMinutes(30);

    @KafkaListener(topics = "location-events", groupId = "lbs-group")
    public void consumeLocation(LocationRequest request) {
        if (isInvalid(request)) return;

        // Redis Geo Indexing 및 상태 정보 저장
        redisTemplate.opsForGeo().add(GEO_KEY, new Point(request.getLongitude(), request.getLatitude()), request.getUserId());
        redisTemplate.opsForValue().set(STATUS_PREFIX + request.getUserId(), request, STATUS_TTL);

        // 지연 시간 및 정확도 계산
        long lag = System.currentTimeMillis() - (request.getTimestamp() != null ? request.getTimestamp() : System.currentTimeMillis());
        log.info(">>> [Consumer] 처리 완료: 유저={} | 지연={}ms | 정확도={}", 
                 request.getUserId(), lag, convertToPercentage(request.getAccuracy()));
    }

    private String convertToPercentage(Double accuracy) {
        if (accuracy == null) return "0%";
        double score = Math.max(0, 100 - (accuracy * 5));
        return String.format("%.0f%%", score);
    }

    private boolean isInvalid(LocationRequest request) {
        Double lat = request.getLatitude();
        Double lon = request.getLongitude();
        return lat < -90 || lat > 90 || lon < -180 || lon > 180;
    }
}
```
</br>

**Step 2. 데이터 품질 수치화**

수신된 GPS 정확도 미터 값을 시스템에서 직관적으로 이해할 수 있는 신뢰도 퍼센티지로 변환하는 로직을 구현했다.

$$
Score=max(0,100−(accuracy×5))
$$

이 공식을 통해 정확도가 0미터일 때는 100퍼센트, 20m 이상으로 벌어질 때는 0퍼센트로 환산하여 데이터의 신뢰성을 정량적으로 평가할 수 있게 했다.

</br></br>

### 4. 데이터 계약서: 엄격한 유효성 검사 (LocationRequest.java)

시스템 전체에서 공유되는 DTO는 일종의 법전과 같다. `@DecimalMin`과 `@PositiveOrZero` 같은 어노테이션을 통해 위도가 90도를 넘거나 속도가 음수가 되는 물리적 오류를 원천 차단했다. 이는 잘못된 데이터가 시스템 깊숙이 침투하여 로직 장애를 일으키는 것을 방지하는 최소한의 방어선이다.

```java
@Getter
@NoArgsConstructor
@AllArgsConstructor
@Builder
public class LocationRequest {
    @NotBlank(message = "사용자 ID는 필수입니다.")
    private String userId;

    @NotNull(message = "위도는 필수입니다.")
    @DecimalMin("-90.0") @DecimalMax("90.0")
    private Double latitude;

    @NotNull(message = "경도는 필수입니다.")
    @DecimalMin("-180.0") @DecimalMax("180.0")
    private Double longitude;

    private Double accuracy;
    private Double speed;
    private String status;

    @Builder.Default
    private Long timestamp = System.currentTimeMillis();
}
```

</br></br>

### 5. 결론: 이벤트 기반 아키텍처가 주는 유연성

<img width="8192" height="5437" alt="mermaid-ai-diagram-2026-01-27-075332" src="https://github.com/user-attachments/assets/9b7e6456-d95d-4e56-9481-08ec1f24710c" />

이번 프로젝트를 통해 입구에서 출구까지 이어지는 이벤트 기반 아키텍처의 정석을 구현해 보았다. 
Kafka라는 완충 지대를 둠으로써 생산자와 소비자 사이의 속도 차이를 조절할 수 있게 되었고, Redis를 통해 실시간 공간 데이터 처리 성능을 극대화했다. 단순히 코드를 짜는 것을 넘어, 각 컴포넌트가 맡은 역할과 데이터의 흐름을 논리적으로 설계하는 과정이 시스템의 전체적인 안정성을 결정한다는 사실을 다시금 체감했다. 

</br></br>

### + 6. 인프라 격리성과 안정성을 고려한 Docker Compose 설계

> 애플리케이션이 제 성능을 발휘하려면 이를 지탱하는 인프라가 견고해야 한다. Docker Compose를 통해 Zookeeper, Kafka, Redis를 구축했으며, 특히 리뷰 피드백을 반영하여 헬스체크 로직의 신뢰성을 높이는 리팩토링을 진행했다.
> 

</br>

### 6-1 . 인프라 구성도 (docker-compose.yml)

```yaml
version: '3.8'

services:
  # 1. Zookeeper: Kafka의 메타데이터 관리 및 클러스터 상태 조율
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    healthcheck:
      test: [ "CMD", "nc", "-z", "localhost", "2181" ]
      interval: 10s
      timeout: 5s
      retries: 5

  # 2. Kafka: 고빈도 위치 이벤트를 수집하고 전달하는 스트리밍 브로커
  kafka:
    image: confluentinc/cp-kafka:7.5.0
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "9092:9092"
    restart: always
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_LISTENERS: INTERNAL://0.0.0.0:29092,EXTERNAL://0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka:29092,EXTERNAL://${KAFKA_EXTERNAL_HOST:-localhost}:9092
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
    healthcheck:
      test: ["CMD", "kafka-topics.sh", "--bootstrap-server", "localhost:29092", "--list"]
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 40s

  # 3. Redis: 최신 위치 상태를 유지하는 고속 인메모리 DB
  redis:
    image: redis/redis-stack:7.4.0-v6
    ports:
      - "6379:6379"
      - "127.0.0.1:8001:8001"
    command: redis-server --save ""
```

</br>

### 6-2. 핵심 리팩토링: INTERNAL 리스너 기반의 헬스체크

기존 설정에서는 Kafka의 헬스체크 포트를 외부 클라이언트용인 9092로 바라보고 있었다. 하지만 이는 환경 변화에 취약한 구조였기에 내부 통신 전용인 29092 포트로 수정했다.

**왜 INTERNAL(29092) 포트를 사용해야 하는가?**

- **설정 간섭 방지:** 외부 리스너는 도메인이나 실제 IP 주소와 같은 환경 변수에 의존한다. 만약 배포 환경에서 외부 주소 설정이 변경되더라도, 컨뉴이너 내부의 자가 진단 로직은 localhost로 고정된 내부 리스너를 바라봄으로써 영향을 받지 않는다.
- **아키텍처의 일관성:** 이미 `KAFKA_INTER_BROKER_LISTENER_NAME` 설정을 통해 내부 통신 규약을 INTERNAL로 정의했다. 헬스체크 역시 관리 목적의 내부 통신에 해당하므로 이 흐름을 따르는 것이 논리적으로 타당하다.
- **신뢰성 확보:** 도커 네트워크 내부에서는 리스너 이름이 보장되므로, 호스트 OS와의 포트 매핑이 달라지는 상황에서도 헬스체크 로직을 수정할 필요가 없는 높은 이식성을 갖추게 된다.

</br>

### 6-3. 각 서비스의 역할과 설계 의도

<img width="2014" height="1310" alt="mermaid-diagram-2026-01-28-010618" src="https://github.com/user-attachments/assets/c1eed641-cdec-4025-b91e-650a375d8b37" />


**Zookeeper: 시스템의 지휘자**
Kafka 의 '뇌' 역할을 하며 브로커들의 상태를 조율한다. `nc -z` 명령어를 통한 헬스체크를 도입하여 Zookeeper가 완전히 준비된 상태에서만 Kafka가 기동되도록 순서를 강제했다.

**Kafka: 안정성 강화 설정**
싱글 브로커 환경에서 발생할 수 있는 오류를 막기 위해 `REPLICATION_FACTOR`를 모두 1로 조정했다. 이는 카프카가 기본적으로 요구하는 복제본 개수 조건을 충족하지 못해 발생하는 서비스 중단을 방지한다.

이중 리스너 설정: `INTERNAL`과 `EXTERNAL`을 구분

- INTERNAL (29092): 도커 컨테이너 환경 내부의 다른 서비스(예: 컨슈머 앱)가 통신할 때 사용
- EXTERNAL (9092): 개발자 PC의 IDE나 외부 라이브러리에서 접속할 때 사용

**Redis: 메모리 기반 초저지연 전략**

`redis-stack` 이미지를 사용하여 브라우저에서 바로 데이터 구조를 확인할 수 있게되어, 데이터 모니터링 편의성을 높였다. 또한 `--save ""` 옵션을 통해 디스크 스냅샷 생성을 비활성화했다. 이는 잦은 쓰기 작업이 발생하는 실시간 위치 시스템에서 디스크 I/O 병목으로 인한 지연 시간을 제거하기 위한 결정이다.

</br></br>

### 7. 마치며

코드 한 줄의 변경이 시스템 전체의 격리성과 안정성을 어떻게 바꾸는지 확인하는 과정이었다. 특히 헬스체크 포트 하나를 정하는 과정에서도 외부 환경과의 의존성을 끊어내려는 고민이 인프라 설계의 핵심임을 깨달았다. 견고하게 구축된 이 인프라 위에서 위치 정보 데이터는 이제 유실 없이 안전하게 흐를 것이다.






